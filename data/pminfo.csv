id_or_path,name,created_at,task_types,description,n_params,input_type,output_type,quant_type,size,last_modified_at,mtype,context_len,license,n_srv_instances,config,eval_metric,eval_score
deepseek-r1:1.5b,deepseek-r1:1.5b,2025-11-04 06:56:38,Text Generation,"DeepSeek-R1 is a family of open reasoning models with performance approaching that of leading models, such as O3 and Gemini 2.5 Pro.",1500000000,Text,Text,-,1100000000,2025-11-04 06:56:38,ollama,128000,Apache-2.0,1,{},"Average of MMLU-Redux, GPQA-Diamond",71.7
gemma3:1b,gemma3:1b,2025-11-04 06:56:39,Text Generation,"Gemma is a lightweight, family of models from Google built on Gemini technology. The Gemma 3 models are multimodal—processing text and images—and feature a 128K context window with support for over 140 languages.",1000000000,Text,Text,-,815000000,2025-11-04 06:56:39,ollama,32000,Apache-2.1,1,{},"Average of MMLU-Redux, GPQA-Diamond",67.8
llama3.2:1b,llama3.2:1b,2025-11-04 06:56:40,Text Generation,The Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). ,1000000000,Text,Text,-,1300000000,2025-11-04 06:56:40,ollama,128000,Apache-2.2,1,{},"Average of MMLU-Redux, GPQA-Diamond",70.1
qwen2.5:0.5b,qwen2.5:0.5b,2025-11-04 06:56:41,Text Generation,"Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support.",500000000,Text,Text,-,398000000,2025-11-04 06:56:41,ollama,32000,Apache-2.3,1,{},"Average of MMLU-Redux, GPQA-Diamond",70.4
phi3:3.8b,phi3:3.8b,2025-11-04 06:56:42,Text Generation,Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft.,3800000000,Text,Text,-,2200000000,2025-11-04 06:56:42,ollama,128000,Apache-2.4,1,{},"Average of MMLU-Redux, GPQA-Diamond",65.8
qwen2.5-coder:0.5b,qwen2.5-coder:0.5b,2025-11-04 06:56:43,Text Generation,"The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.",500000000,Text,Text,-,398000000,2025-11-04 06:56:43,ollama,32000,Apache-2.5,1,{},"Average of MMLU-Redux, GPQA-Diamond",70.9
tinyllama:1.1b,tinyllama:1.1b,2025-11-04 06:56:44,Text Generation,The TinyLlama project is an open endeavor to train a compact 1.1B Llama model on 3 trillion tokens.,1000000000,Text,Text,-,638000000,2025-11-04 06:56:44,ollama,2000,Apache-2.6,1,{},"Average of MMLU-Redux, GPQA-Diamond",63.5
starcoder2:3b,starcoder2:3b,2025-11-04 06:56:45,Text Generation,"StarCoder2 is the next generation of transparently trained open code LLMs that comes in three sizes: 3B, 7B and 15B parameters.",3000000000,Text,Text,-,1700000000,2025-11-04 06:56:45,ollama,16000,Apache-2.7,1,{},"Average of MMLU-Redux, GPQA-Diamond",68.2
granite3.1-moe:1b,granite3.1-moe:1b,2025-11-04 06:56:46,Text Generation,The IBM Granite 1B and 3B models are long-context mixture of experts (MoE) Granite models from IBM designed for low latency usage.,1000000000,Text,Text,-,1400000000,2025-11-04 06:56:46,ollama,128000,Apache-2.8,1,{},"Average of MMLU-Redux, GPQA-Diamond",62.4
falcon3:1b,falcon3:1b,2025-11-04 06:56:47,Text Generation,"A family of efficient AI models under 10B parameters performant in science, math, and coding through innovative training techniques.",1000000000,Text,Text,-,1800000000,2025-11-04 06:56:47,ollama,8000,Apache-2.9,1,{},"Average of MMLU-Redux, GPQA-Diamond",63.8
